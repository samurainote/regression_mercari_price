# -*- coding: utf-8 -*-
"""mercari_5th_main_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10S7JThNUq_ujri7D-UHyNCO8rs01GLZM

# Regression with ML: Mercari Price Prediction

## Goal
To find best price for each second-hand products which users uplod on mercari   
## 1. Data Preparation
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = "https://drive.google.com/open?id=1fT2O4okInxqZ0quXJkI4oJfLjK1Q_tQH"

fluff, id = link.split('=')
print (id)

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile("train.tsv")  
mercari = pd.read_csv("train.tsv", delimiter="\t")

submission = pd.read_csv("sample_submission.csv")
submission.head()

#mercari = pd.read_csv("train.tsv", delimiter="\t")
mercari.head()

mercari_test = pd.read_csv("test.tsv", delimiter="\t")
mercari_test.head()

"""## 2. Target and Feature Split"""

mercari_price = mercari[["price", "train_id"]]
mercari_price.head()

mercari_price.shape

mercari_price.drop(labels=[511535, 861230, 1224924, 1264242], inplace=True)
mercari_price.reset_index(drop=True, inplace=True)
mercari_price.shape

mercari.drop("price", axis=1, inplace=True)
mercari.head()

mercari.shape

"""## 3. Convert Data Type"""

mercari.dtypes

len(mercari["brand_name"].unique())

"""## 4. Missing Value"""

mercari.isnull().any()

mercari.isnull().sum()

"""I will cut 4 records which have null value in column "item_description"."""

mercari[mercari["item_description"].isnull()]

mercari.drop(labels=[511535, 861230, 1224924, 1264242], inplace=True)
mercari.shape

mercari["brand_name"].fillna(value="no_brand", inplace=True)

mercari["category_name"].fillna(value="no_category", inplace=True)

mercari.isnull().sum()

len(mercari)

mercari.dropna(inplace=True)
len(mercari)
mercari.isnull().sum()

"""# Policy for each Features
1. Sequencial  Category Data
    - item_condition_id -> MultiLabelEncode
    
    
2. Discrete Category Data
    - shipping -> Binary
    - category_name -> Multiple-One-Hot
    - brand_name -> One-Hot
    
      
3. NLP Data
    - name
    - item_description

## 5. Numerical Values: Outlier Exclusion
"""

mercari_price.head()

mercari_price.isnull().sum()

plt.figure(figsize=(14,8))
sns.distplot(mercari_price["price"], hist_kws={"color": "Orange"}, kde_kws={"color": "Navy"})
plt.show();

# the condition of the items provided by the seller
sns.set(style="darkgrid")
ax = sns.countplot(x="item_condition_id", data=mercari)

# 1 if shipping fee is paid by seller and 0 by buyer
ax = sns.countplot(x="shipping", data=mercari, palette="husl")

# correlation with above 2 features
numeric_category = ["item_condition_id", "shipping"]
cor = mercari[numeric_category].corr()
sns.heatmap(cor, annot=True)

"""### column "category_name" 1 to 5"""

max_length = 3
counter = 0
greater_index_lists = []

for category_text in mercari["category_name"]:
    if category_text == "no_category":
        counter += 1
        continue
    else:
        category_list = category_text.split("/")        
        if len(category_list) > 3:
            greater_index_lists.append(counter) 
            counter += 1
            if len(category_list) > max_length:
                max_length = len(category_list)
        else:
            counter += 1

len(greater_index_lists)

counter

max_length

cat1 = []
cat2 = []
cat3 = []
cat4 = []
cat5 = []
counter = 0

for category_text in mercari["category_name"]:
    
    if category_text == "no_category":
        cat1.append(np.nan)
        cat2.append(np.nan)
        cat3.append(np.nan)
        cat4.append(np.nan)
        cat5.append(np.nan)
        counter += 1
    else:
        category_list = category_text.split("/")
        
        if len(category_list) == 3:
            cat1.append(category_list[0])
            cat2.append(category_list[1])
            cat3.append(category_list[2])
            cat4.append(np.nan)
            cat5.append(np.nan)
            counter += 1
        elif len(category_list) == 4:
            cat1.append(category_list[0])
            cat2.append(category_list[1])
            cat3.append(category_list[2])
            cat4.append(category_list[3])
            cat5.append(np.nan)
            counter += 1       
        elif len(category_list) == 5:
            cat1.append(category_list[0])
            cat2.append(category_list[1])
            cat3.append(category_list[2])
            cat4.append(category_list[3])
            cat5.append(category_list[4])
            counter += 1
        else:
            print("error")
            break
            
new_category = pd.DataFrame({"category_A": cat1, "category_B": cat2, "category_C": cat3, "category_D": cat4, "category_E": cat5})
#len(new_category)
counter

new_category.shape

mercari.drop("category_name", axis=1, inplace=True)

mercari.shape

mercari.reset_index(drop=True, inplace=True)

mercari_cat = pd.concat([mercari, new_category], axis=1)
mercari_cat.shape

mercari_cat.head()

a = list(mercari_cat["category_A"].unique())
len(a)

b = list(mercari_cat["category_B"].unique())
len(b)

for i in range(0, len(a)):
    if a[i] in b:
        print("duplicates!")

df_A = pd.concat([mercari_cat["category_A"], mercari_price], axis=1)
df_A.head()

df_A.shape

plt.rcParams['figure.figsize'] = (400.0, 200.0)

"""It looks like category_A has some relationship with Price."""

# plt.figure(figsize=(2000, 800))
sns.catplot(x="category_A", y="price", kind="box", data=df_A);
plt.show();

df_B = pd.concat([mercari_cat["category_B"], mercari_price], axis=1)
df_B.head()

plt.figure(figsize=(20, 12))
sns.catplot(x="category_B", y="price", kind="swarm", data=df_B);
plt.show();

"""## What I found till now about features
Slightly related with Price  
- item_condition_id
- shipping
   
Generally related with Price  
- category_name (SVD)
- brand_name ()

Not yet checked (tf-idf -> word2vec -> t-SNE)
- name
- item_description

### for "brand_name", I will use t-SNE/PCA to reduce about 5k categories to 2D/3D
"""

# count unique value
len(mercari["brand_name"].unique())

"""I will check the price distribution which has brand or not"""

len(mercari["brand_name"])

index_num = 0
no_brand_index = []
brand_index = []

for v in mercari["brand_name"]:
    if v == "no_brand":
        no_brand_index.append(index_num)
        index_num += 1
    else:
        brand_index.append(index_num)
        index_num += 1

print("Length of no_brand_index: ", len(no_brand_index))
print("Length of brand_index: ", len(brand_index))

sns.distplot(mercari.ix[no_brand_index, "brand_name"], color="skyblue", label="No Brand")
sns.distplot(mercari.ix[brand_index, "brand_name"], color="red", label="Brand Products")
sns.plt.legend()

"""# Preorocessing for NLP

I will treat two features "name" and "item_description" as text data.

#### Process
1. Concat "name" and "item_description"
2. Remove all irrelevant characters such as 24 or @
3. Tokenize your text by separating it into individual words
4. Normalize all characters to lowercase, in order to treat words such as “hello”, “Hello”, and “HELLO” the same
5. Lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)
6. Remove "stopwords"
7. Bag of words: TF-IDF
8. Dimensional Reduction: t-SNE and LDA

"name" and "item_description" -> BoW -> 2D or 3D

-----------------------------

* Bag of Words: CountVectorizer & TF-IDF is scaler: from word to scale
* Word Embedding: word2vec, doc2vec, seq2seq is semantic vector
"""

mercari[["name", "item_description"]].head(20)

mercari[["name", "item_description"]].isnull().sum()

"""In description, some data have "No description yet". I will remove this as stopword before."""

no_desc = mercari[mercari["item_description"]=="No description yet"].index

type(no_desc)

len(no_desc)

no_desc_list = list(no_desc)

mercari.ix[no_desc_list, "item_description"] = np.nan

mercari.isnull().sum()

mercari["item_description"].fillna("", inplace=True)

mercari[["name", "item_description"]].head(20)

mercari_text = pd.concat([mercari["name"], mercari["item_description"]], axis=1)
mercari_text.head()

len(mercari_text)

name_list = []
desc_list = []

for name in mercari_text["name"]:
    name_list.append(name)

for desc in mercari_text["item_description"]:
    desc_list.append(desc)

len(name_list)

len(desc_list)

merge_text_list = []

for i in range(0, len(mercari_text)):
    merge_text = name_list[i] + " " + desc_list[i]
    merge_text_list.append(merge_text)
    
len(merge_text_list)

text_df = pd.DataFrame(merge_text_list, columns=["text"])
text_df.head()

len(merge_text_list)

"""2. Normalize all characters to lowercase, in order to treat words such as “hello”, “Hello”, and “HELLO” the same"""

text_df["text"] = text_df["text"].apply(lambda x: " ".join(x.lower() for x in x.split()))
text_df["text"].head()

"""3. Remove all irrelevant characters: numbers, mark, emoji such as 24 or @"""

import re
import string

text_df["text"].head(20)

text_df["text"] = text_df["text"].str.replace('[^\w\s]','')
text_df["text"].head(20)

text_df["text"] = text_df["text"].str.replace(r"\d+", "")
text_df["text"].head()

text_df["text"] = text_df["text"].str.replace(r"[︰-＠]", "")
text_df["text"].head(20)

"""4. Tokenize your text by separating it into individual words"""

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

text_df["text"] = text_df["text"].apply(word_tokenize)
text_df["text"].head()

"""5. Remove "stopwords""""

nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words("english")
text_df["text_nonstop"] = text_df["text"].apply(lambda x: [item for item in x if item not in stop_words])
text_df["text_nonstop"].head(20)

"""6. Stemming and Lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)"""

from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")

text_df["stemmed"] = text_df["text_nonstop"].apply(lambda x: [stemmer.stem(e) for e in x])
text_df["stemmed"].head(10)

nltk.download('wordnet')

# Lemmatization
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
text_df["lemmatized"] = text_df["stemmed"].apply(lambda x: [lemmatizer.lemmatize(e) for e in x])
text_df["lemmatized"].head(10)

text_df.head()

import sys
import pprint

pprint.pprint(sys.path)

pwd

text_df.to_csv('/content/text_df.csv')

mercari.to_csv('/content/mercari.csv')

ls

text_df = pd.read_csv("text_df.csv")
text_df.head()

"""7. Bag of words: TF-IDF"""

text_df["lemmatized"].dtypes

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=0.03)
text_df["tfidf"] = [" ".join(e) for e in text_df["lemmatized"].values]

text_df["tfidf"][400:450]

text_df.iloc[0,1]

text_df.to_csv('/content/text_df_tfidf.csv')

x = "['mlb', 'cincinnati', 'reds', 't', 'shirt', 'size', 'xl']"

type(x)

text_df["text"]

pwd

"""問題点：ストップワードを除去したためにテキスト要素がない"[""]"データがtfidfベクトルを通過しない
よってストップワードを事前にせず、tfidfvectorizerに内包することで解決
csv保存ずみ
"""

text_df = pd.read_csv("text_df.csv")
text_df.head()

# Lemmatization
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
text_df["stem"] = text_df["text"].apply(lambda x: [stemmer.stem(e) for e in x])
text_df["len_without_stop"] = text_df["stem"].apply(lambda x: [lemmatizer.lemmatize(e) for e in x])
text_df["len_without_stop"].head(10)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=0.03, stop_words='english')
#text_df["tfidf"] = [" ".join(e) for e in text_df["lemmatized"].values]

tfidf_X = vectorizer.fit_transform(list(text_df["len_without_stop"].values)).toarray()







counter = 0

for string in text_df["tfidf"]:
  tfidf_X = vectorizer.fit_transform(text_df["tfidf"]).toarray()
  counter += 1

counter

all_desc = np.append(train['item_description'].values, test['item_description'].values)
tfidf_X = vectorizer.fit_transform(list(all_desc)).toarray()

tfidf_X = vectorizer.fit_transform(list(text_df["len_without_stop"].values)).toarray()





index = tfidf_X.argsort(axis=1)[:,::-1]
feature_names = np.array(vectorizer.get_feature_names())
feature_words = feature_names[index]

n = 3
tfidf_top3 = []
for fwords in feature_words[:,:n]:

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.datasets import fetch_20newsgroups

news20 = fetch_20newsgroups()
vectorizer = TfidfVectorizer(min_df=0.03)
tfidf_X = vectorizer.fit_transform(news20.data[:1000]).toarray()

index = tfidf_X.argsort(axis=1)[:,::-1]
feature_names = np.array(vectorizer.get_feature_names())
feature_words = feature_names[index]

n = 5  # top何単語取るか
m = 15  # 何記事サンプルとして抽出するか
for fwords, target in zip(feature_words[:m,:n], news20.target):
    # 各文書ごとにtarget（ラベル）とtop nの重要語を表示
    print(news20.target_names[target])
    print(fwords)



"""8. Dimensional Reduction: t-SNE and LDA"""

stop





from sklearn.manifold import TSNE
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
X_embedded = TSNE(n_components=2).fit_transform(X)
X_embedded.shape

# MLPClassifier

"""# Stacking with RMRegressor, XGBoost, NeuralNetwork

- Tf-idf: Feature Extraction for text data (BoW)
- word2vec: Vctorize from word to numerical data
- t-SNE: Dimensional Reduction

# ベクトル化しないで次元削除ってどういうこと？
## tf-idf -> t-SNE

countvectorizer & LDA SVD
"""

